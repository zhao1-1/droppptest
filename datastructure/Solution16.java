package datastructure;

/**
 * @author bin2.zhao (D52B48 in ZhangMen)
 * @since 2021/11/3 14:23
 */

/**
 * 海量数据处理
 */
public class Solution16 {

    /*

    海量数据处理面对两个问题：
        1. 一台机器的内存存不了；
        2. 一台机器处理的太慢；


    海量数据处理的核心思想：分治
        + 单机：利用外部存储，分批加入内存处理；
        + 多机：对数据分片，利用多机内存存储；
        + 多机：并行计算，利用多线程、多机并行处理；


    常见问题：
        1. 海量数据排序；
        2. 海量数据查询；
        3. 海量数据TOP-K；
        4. 海量数据求出现频率的TOP-K；
        5. 海量数据去重/找重；
        6. 两海量文件找重；


    处理技巧：
        +  先考虑在非海量的情况下如何处理，再调整方案适配海量数据的处理；
        A. 外部排序：多路归并、桶排序；
        B. 哈希分片
        C. 位图

     */


    /**
     *【16-1】海量数据排序（离线处理）
     * 我们有 10GB 的订单数据，
       我们希望按订单金额（假设金额都是正整数）进行排序，
       但是我们的内存有限，只有200MB，没办法一次性把 10GB 的数据都加载到内存中。这个时候该怎么办呢？
     * 解法一：多路归并
     * 缺点：频繁，大量的IO操作
     */
    /*
    一、分批排序：
        1. 将10GB文件，每100MB分一组，分批加载到内存中进行排序；
        2. 将分批排好序的这100个文件，按照1.txt ~ 100.txt进行命名，再存储到硬盘中；
    二、多个有序文件合并
        3. 将这100个硬盘中已经排好序的文件，按照1MB大小一组，每个文件划分100个缓存组；
        4. 讲这100个缓存组（每个缓存组1MB）同时加载进内存；
        5. 将内存中这100个缓存组按照｛合并多个有序数组的算法｝，依次写入硬盘result.txt；
        6. 当其中一个缓存组全部被处理完毕后，再从对应的文件中加载下一个缓存组进内存，继续｛合并多个有序数组｝；
    三、升级：
        + 以上都是单机，我们可以多机同时处理100个文件的排序，但是归并处理还是得单机；
     */


    /**
     *【16-1】海量数据排序
     * 解法二：桶排序
     * 要求：数据分布比较均匀，如果数据分布呈正态分布等分均匀分布，则不适用桶排序
     */
    /*
    一、分桶：
        1. 比如这10GB订单的金额分布是在0 ~ 9999之间；
        2. 我们在硬盘内划分100个【桶】，每个"桶文件"命名为1.txt ~ 100.txt；
        3. 桶文件存储的订单金额范围是：[0-99], [100-199], [200-299],....[9800-9899], [9900-9999]
        4. 扫描争哥订单文件，按照金额，分布将数据放到对应的桶中。比如扫描到一条记录是订单金额为4588元，则将该记录写进45.txt桶中。
    二、对桶内数据排序：
        5. 利用多台机器，对桶内数据分别进行排序，得到排好序的100个桶（100个txt文件）；
    三、合并数据：
        6. 依次将这100个排好序的桶文件顺序合并为一个大文件，则是最后的结果。
     */


    /**
     *【16-2】海量数据搜索（实时服务）
     * 有一个IP地址白名单文件，包含10亿个IP地址，判断某IP是否在白名单中？
     * 解法一：位图
     */

    /**
     *【16-2】海量数据搜索（实时服务）
     * 解法二：多机分片
     */


    /**
     *【16-3】海量数据查询
     * 10亿个整数，判断某个整数是否在其中？
     */


    /**
     *【16-4】海量数据TOP-K
     * 10亿个整数，放在文件中，内存有限，如何求最大的TOP100个整数？
     * 解法一：堆排序
     */
    /*
        1. 内存中构建一个小顶堆；
        2. 一次从文件中选取100MB数据进内存，维护小顶堆；
     */


    /**
     *【16-5】海量数据求出现频率的TOP-K
     * 100GB的搜索关键字文件，统计出现频率TOP100关键词。
     */


    /**
     *【16-6】海量数据去重
     * 一个文件中包含10亿条URL，有可能有重复，将重复的去掉。
       假设平均每个URL占用64字节，10亿条数据占用大约 = 64 * 10 ^ 9 = 64 GB
     */
    /*
    先预估，看一下内存能否存下？

    + 内存中可以放下的情况：
        解法一：排序；
        解法二：hash；

    + 内存不够的情况：
        解法一：海量数据排序；
        1. 利用海量数据排序，得到10亿条排好序的URL文件；
        2. 开辟100MB的缓存空间，每次从文件中读取一批去重（把每批处理的最后一个URL加到一个小缓存里，重新读的一批文件开头URL先跟之前的尾巴进行比较），再写入结果文件中；

        解法二：分片hash；
        1. 对文件中的每个URL求MD5(URL)%8 = 0 ~ 7；
        2. 对计算结果是0的全部放到内存里；
        3. 对这批数据去重，去重后放到小文件里；
        4. 合并这些小文件即可。
     */


    /**
     *【16-7】两海量文件找重
     * 给你a、b两个文件，各自有50亿条URL，每条URL占用64字节，内存限制是4GB，找出a、b文件中共同的URL。
     */
    /*
    + 内存可以放心全部数据：
        解法一：排序 + 双指针；
        解法二：hash表

    + 内存不够的情况：
        解法一：海量数据处理
        1. 对两个文件分布海量数据排序；
        2. 每个文件都需要维护一个offset偏移量；
        3. 每个文件选取1GB缓存，将两个文件的缓存放到内存里进行去重；

        解法二：hash分片
        1. 比如分片为8，则需要扫描8次两个文件，依次把hash结果为0，1，2....7的放进内存中进行找重处理；
     */


}
